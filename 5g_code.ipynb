{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.环境和数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  以下是所有需要用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T02:05:56.779990Z",
     "iopub.status.busy": "2022-02-16T02:05:56.779395Z",
     "iopub.status.idle": "2022-02-16T02:05:58.510899Z",
     "shell.execute_reply": "2022-02-16T02:05:58.510499Z",
     "shell.execute_reply.started": "2022-02-16T02:05:56.779581Z"
    },
    "id": "YG6b2j9EMSAe",
    "outputId": "d8602946-cae9-4d37-f343-5f9991008c88",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "import timm\n",
    "import h5py\n",
    "import os\n",
    "import random\n",
    "import scipy.io as scio\n",
    "import matplotlib.pyplot as plt\n",
    "#!pip install torchsummary\n",
    "#!pip install timm\n",
    "\n",
    "#gpu_list = '0'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_list\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYHTONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "def visualize(img):\n",
    "    if len(img.shape) == 2:\n",
    "        plt.imshow(img, cmap='gray', vmin = 0, vmax = 1,interpolation='none')\n",
    "        plt.show()\n",
    "    else:\n",
    "        for i in range(2):\n",
    "            plt.imshow(img[i,:,:,], cmap='gray', vmin = 0, vmax = 1,interpolation='none')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T02:06:09.109771Z",
     "iopub.status.busy": "2022-02-16T02:06:09.109434Z",
     "iopub.status.idle": "2022-02-16T02:06:09.148996Z",
     "shell.execute_reply": "2022-02-16T02:06:09.148538Z",
     "shell.execute_reply.started": "2022-02-16T02:06:09.109753Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T02:06:11.082869Z",
     "iopub.status.busy": "2022-02-16T02:06:11.082588Z",
     "iopub.status.idle": "2022-02-16T02:06:11.086389Z",
     "shell.execute_reply": "2022-02-16T02:06:11.086035Z",
     "shell.execute_reply.started": "2022-02-16T02:06:11.082851Z"
    },
    "id": "3NMYLtQvsCwa",
    "outputId": "517778fe-a254-4ec8-c8a5-d3078c925c37",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T02:06:23.312674Z",
     "iopub.status.busy": "2022-02-16T02:06:23.312479Z",
     "iopub.status.idle": "2022-02-16T02:06:23.560562Z",
     "shell.execute_reply": "2022-02-16T02:06:23.560158Z",
     "shell.execute_reply.started": "2022-02-16T02:06:23.312657Z"
    },
    "id": "aOFgfnXLVfHH",
    "outputId": "ff0fceb6-7d3a-4675-9b64-4230fcb419df",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/python_prj/5g\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 训练数据加载和探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T02:23:04.879378Z",
     "iopub.status.busy": "2022-02-16T02:23:04.879164Z",
     "iopub.status.idle": "2022-02-16T02:23:12.270411Z",
     "shell.execute_reply": "2022-02-16T02:23:12.270047Z",
     "shell.execute_reply.started": "2022-02-16T02:23:04.879361Z"
    },
    "id": "XDStaqaRNYhd",
    "outputId": "bea191e3-d411-4df0-af12-785b69edc02d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2, 126, 128)\n",
      "(2000, 2, 126, 128)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "data_load_address = './data/'\n",
    "mat = scio.loadmat(data_load_address+'/Htrain.mat')\n",
    "x_train = mat['H_train']  # shape=8000*126*128*2\n",
    "\n",
    "x_train = np.transpose(x_train.astype('float32'),[0,3,1,2])\n",
    "print(np.shape(x_train))\n",
    "\n",
    "mat = scio.loadmat(data_load_address+'/Htest.mat')\n",
    "x_test = mat['H_test']  # shape=2000*126*128*2\n",
    "\n",
    "x_test = np.transpose(x_test.astype('float32'),[0,3,1,2])\n",
    "print(np.shape(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T02:30:57.129697Z",
     "iopub.status.busy": "2022-02-16T02:30:57.129511Z",
     "iopub.status.idle": "2022-02-16T02:30:57.132193Z",
     "shell.execute_reply": "2022-02-16T02:30:57.131668Z",
     "shell.execute_reply.started": "2022-02-16T02:30:57.129679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#visualize(1-(x_test[219]-0.5)/0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.模型思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型主要参考了两篇论文，\n",
    "(使用CRNet网络,channel reconstruction network) 和 MRFnet。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文①:https://arxiv.org/pdf/1910.14322.pdf\n",
    "<br>中的思路，它设计了一种多分辨率结构的网络，对'信道图'进行编码和解码,\n",
    "<br>这种结构在没有任何额外信息的情况下，在相同的计算复杂度下，所提出的CRNet优于最先进的CsiNet\n",
    "<br>将多分辨路径和卷积分解引入CSI反馈任务，并设计了一个退火训练方案优化CRNet性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T03:21:50.230439Z",
     "iopub.status.busy": "2022-02-16T03:21:50.230196Z",
     "iopub.status.idle": "2022-02-16T03:21:50.234387Z",
     "shell.execute_reply": "2022-02-16T03:21:50.233688Z",
     "shell.execute_reply.started": "2022-02-16T03:21:50.230411Z"
    }
   },
   "source": [
    "还有论文②：https://ieeexplore.ieee.org/document/9495802/ 中提出的MRFNet的结构。\n",
    "<br>MRFNet可以恢复具有不同感受野和大量卷积通道的丰富特征，从而更好地恢复CSI。\n",
    "作者用不同数量的卷积通道和多个感受野来可视化最后一个块的输出和MRFNet的最终输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 本文融合了上述两种网络，进行复现。\n",
    "\n",
    "<br>在Encode阶段，使用了CRNet的block结构。\n",
    "<br>在Deconde阶段，使用MRFNet的Block结果。\n",
    "<br> 融合的关键在于使用CRnet的编码阶段网络优化,MRFNet的Encode，进一步提升整体性能,联合了两种网络的优点；\n",
    "<br>并应用于本比赛场景，取得了初步的效果，由于节后开工时间紧迫只提交了一次，后面还待优化，模型代码如下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.model定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T03:28:33.037135Z",
     "iopub.status.busy": "2022-02-16T03:28:33.036698Z",
     "iopub.status.idle": "2022-02-16T03:28:33.039555Z",
     "shell.execute_reply": "2022-02-16T03:28:33.039019Z",
     "shell.execute_reply.started": "2022-02-16T03:28:33.037115Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean = 0.5\n",
    "std = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T03:15:44.708460Z",
     "iopub.status.busy": "2022-02-16T03:15:44.708194Z",
     "iopub.status.idle": "2022-02-16T03:15:44.720405Z",
     "shell.execute_reply": "2022-02-16T03:15:44.719884Z",
     "shell.execute_reply.started": "2022-02-16T03:15:44.708442Z"
    },
    "id": "dcQrOqGhNRlN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Num2Bit(Num, B):\n",
    "    Num_ = Num.type(torch.uint8)\n",
    "\n",
    "    def integer2bit(integer, num_bits=B * 2):\n",
    "        dtype = integer.type()\n",
    "        exponent_bits = -torch.arange(-(num_bits - 1), 1).type(dtype)\n",
    "        exponent_bits = exponent_bits.repeat(integer.shape + (1,))\n",
    "        out = integer.unsqueeze(-1) // 2 ** exponent_bits\n",
    "        return (out - (out % 1)) % 2\n",
    "\n",
    "    bit = integer2bit(Num_)\n",
    "    bit = (bit[:, :, B:]).reshape(-1, Num_.shape[1] * B)\n",
    "    return bit.type(torch.float32)\n",
    "\n",
    "\n",
    "def Bit2Num(Bit, B):\n",
    "    Bit_ = Bit.type(torch.float32)\n",
    "    Bit_ = torch.reshape(Bit_, [-1, int(Bit_.shape[1] / B), B])\n",
    "    num = torch.zeros(Bit_[:, :, 1].shape).cuda()\n",
    "    for i in range(B):\n",
    "        num = num + Bit_[:, :, i] * 2 ** (B - 1 - i)\n",
    "    return num\n",
    "\n",
    "\n",
    "class Quantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = torch.round(x * step - 0.5)\n",
    "        out = Num2Bit(out, B)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of constant arguments to forward must be None.\n",
    "        # Gradient of a number is the sum of its B bits.\n",
    "        b, _ = grad_output.shape\n",
    "        grad_num = torch.sum(grad_output.reshape(b, -1, ctx.constant), dim=2) / ctx.constant\n",
    "        return grad_num, None\n",
    "\n",
    "\n",
    "class Dequantization(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, B):\n",
    "        ctx.constant = B\n",
    "        step = 2 ** B\n",
    "        out = Bit2Num(x, B)\n",
    "        out = (out + 0.5) / step\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        # repeat the gradient of a Num for B time.\n",
    "        b, c = grad_output.shape\n",
    "        grad_output = grad_output.unsqueeze(2) / ctx.constant\n",
    "        grad_bit = grad_output.expand(b, c, ctx.constant)\n",
    "        return torch.reshape(grad_bit, (-1, c * ctx.constant)), None\n",
    "\n",
    "\n",
    "class QuantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(QuantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Quantization.apply(x, self.B)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DequantizationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, B):\n",
    "        super(DequantizationLayer, self).__init__()\n",
    "        self.B = B\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = Dequantization.apply(x, self.B)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-16T03:15:46.708163Z",
     "iopub.status.busy": "2022-02-16T03:15:46.707937Z",
     "iopub.status.idle": "2022-02-16T03:15:46.745781Z",
     "shell.execute_reply": "2022-02-16T03:15:46.745361Z",
     "shell.execute_reply.started": "2022-02-16T03:15:46.708145Z"
    },
    "id": "tym-wWxFNYjl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=True)\n",
    "\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, groups=1):\n",
    "        if not isinstance(kernel_size, int):\n",
    "            padding = [(i - 1) // 2 for i in kernel_size]\n",
    "        else:\n",
    "            padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(OrderedDict([\n",
    "            ('conv', nn.Conv2d(in_planes, out_planes, kernel_size, stride,\n",
    "                               padding=padding, groups=groups, bias=False)),\n",
    "            ('bn', nn.BatchNorm2d(out_planes))\n",
    "        ]))\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=8):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.BatchNorm1d(channel // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.BatchNorm1d(channel ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class CRBlock(nn.Module):\n",
    "    def __init__(self,ch_nums,norm=True):\n",
    "        super(CRBlock, self).__init__()\n",
    "        self.path1 = nn.Sequential(OrderedDict([\n",
    "            ('conv3x3', ConvBN(2, ch_nums, 3)),\n",
    "            ('relu1', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv1x9', ConvBN(ch_nums, ch_nums, [1, 9])),\n",
    "            ('relu2', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv9x1', ConvBN(ch_nums, ch_nums, [9, 1])),\n",
    "        ]))\n",
    "        self.path2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1x5', ConvBN(2, ch_nums, [1, 5])),\n",
    "            ('relu', nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            ('conv5x1', ConvBN(ch_nums, ch_nums, [5, 1])),\n",
    "        ]))\n",
    "        self.identity = nn.Identity()\n",
    "        self.norm = norm\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.3, inplace=True) \n",
    "        self.se = SELayer(ch_nums *2)\n",
    "        if norm:\n",
    "            self.out_layer = nn.Sequential(\n",
    "                nn.BatchNorm2d(2),\n",
    "                nn.LeakyReLU(negative_slope=0.3, inplace=True),\n",
    "            )\n",
    "            self.conv1x1 = ConvBN(ch_nums * 2, 2, 1)\n",
    "            \n",
    "        else:\n",
    "            self.conv1x1 = conv3x3(ch_nums*2,2)\n",
    "            self.out_layer = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out = torch.cat((out1, out2), dim=1)\n",
    "        out = self.se(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1x1(out)\n",
    "        if self.norm:\n",
    "            out = self.out_layer(out + identity)\n",
    "        else:\n",
    "            out = self.out_layer(out)\n",
    "        return out\n",
    "\n",
    "class MRFBlock(nn.Module):\n",
    "    def __init__(self,ch_nums=64):\n",
    "        super(MRFBlock, self).__init__()\n",
    "        self.path1 = nn.Sequential(\n",
    "            ConvBN(ch_nums,ch_nums,5),\n",
    "            nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        )\n",
    "        self.path2 = nn.Sequential(\n",
    "            ConvBN(ch_nums,ch_nums,7),\n",
    "            nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        )\n",
    "        self.path3 = nn.Sequential(\n",
    "            ConvBN(ch_nums,ch_nums,9),\n",
    "            nn.LeakyReLU(negative_slope=0.3, inplace=True)\n",
    "        )\n",
    "        self.conv1x1 = nn.Sequential(\n",
    "            ConvBN(ch_nums*3,ch_nums,1),\n",
    "            nn.BatchNorm2d(ch_nums)\n",
    "        )\n",
    "        self.identity = nn.Identity()\n",
    "        self.relu = nn.LeakyReLU(negative_slope=0.3, inplace=True)               \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.identity(x)\n",
    "\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        out3 = self.path3(x)\n",
    "        out = torch.cat((out1, out2,out3), dim=1)\n",
    "        out = self.conv1x1(out)\n",
    "        out = self.relu(out+identity)\n",
    "        return out\n",
    "    \n",
    "def get_efficientnet_ns(model_name='tf_efficientnet_b0_ns', pretrained=True):\n",
    "    net = timm.create_model(model_name, pretrained=pretrained)\n",
    "    n_features = net.classifier.in_features\n",
    "\n",
    "    return net, n_features\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    B = 4\n",
    "\n",
    "    def __init__(self, feedback_bits, efn_name='tf_efficientnet_b0_ns'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(32256, int(feedback_bits // self.B))\n",
    "        self.bn = nn.BatchNorm1d(int(feedback_bits // self.B))\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 2, 5 )),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"CRBlock1\", CRBlock(32)),\n",
    "        ]))\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.quantize = QuantizationLayer(self.B)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x-mean)/std      \n",
    "        x = self.encoder(x)\n",
    "        out = torch.flatten(x, 1)\n",
    "        out = self.bn(self.fc(out))\n",
    "        out = self.sig(out)\n",
    "        out = self.quantize(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    B = 4\n",
    "\n",
    "    def __init__(self, feedback_bits):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.feedback_bits = feedback_bits\n",
    "        self.dequantize = DequantizationLayer(self.B)\n",
    "        \n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            (\"conv5x5_bn\", ConvBN(2, 64, 3 )),\n",
    "            (\"relu\", nn.LeakyReLU(negative_slope=0.3, inplace=True)),\n",
    "            (\"MRFBlock1\", MRFBlock(64)),\n",
    "            (\"MRFBlock1\", MRFBlock(64)),\n",
    "        ]))\n",
    "        \n",
    "        self.fc = nn.Linear(int(feedback_bits // self.B), 32256)\n",
    "        self.bn1d = nn.BatchNorm1d(32256)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.out_conv = conv3x3(64,2)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.dequantize(x)\n",
    "        out = out.view(-1, int(self.feedback_bits // self.B))\n",
    "\n",
    "        out = self.sig(self.fc(out))\n",
    "        out = out.view(-1, 2, 126, 128)\n",
    "        out = self.decoder(out)\n",
    "        out = self.sig(self.out_conv(out))\n",
    "        return out\n",
    "\n",
    "# Note: Do not modify following class and keep it in your submission.\n",
    "# feedback_bits is 512 by default.\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, feedback_bits, efn_name='tf_efficientnet_b0_ns'):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(feedback_bits, efn_name=efn_name)\n",
    "        self.decoder = Decoder(feedback_bits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = self.encoder(x)\n",
    "        out = self.decoder(feature)\n",
    "        return out\n",
    "\n",
    "    def forward_train(self,x):\n",
    "        feature,en_data = self.encoder.forward_train(x)\n",
    "        out,de_data = self.decoder.forward_train(feature)\n",
    "        return out,en_data,de_data\n",
    "\n",
    "\n",
    "def NMSE(x, x_hat):\n",
    "    x_real = np.reshape(x[:, :, :, 0], (len(x), -1))\n",
    "    x_imag = np.reshape(x[:, :, :, 1], (len(x), -1))\n",
    "    x_hat_real = np.reshape(x_hat[:, :, :, 0], (len(x_hat), -1))\n",
    "    x_hat_imag = np.reshape(x_hat[:, :, :, 1], (len(x_hat), -1))\n",
    "    x_C = x_real - 0.5 + 1j * (x_imag - 0.5)\n",
    "    x_hat_C = x_hat_real - 0.5 + 1j * (x_hat_imag - 0.5)\n",
    "    power = np.sum(abs(x_C) ** 2, axis=1)\n",
    "    mse = np.sum(abs(x_C - x_hat_C) ** 2, axis=1)\n",
    "    nmse = np.mean(mse / power)\n",
    "    return nmse\n",
    "\n",
    "def NMSE_cuda(x, x_hat):\n",
    "    x_real = x[:, 0, :, :].view(len(x), -1) - 0.5\n",
    "    x_imag = x[:, 1, :, :].view(len(x), -1) - 0.5\n",
    "    x_hat_real = x_hat[:, 0, :, :].view(len(x_hat), -1) - 0.5\n",
    "    x_hat_imag = x_hat[:, 1, :, :].view(len(x_hat), -1) - 0.5\n",
    "    power = torch.sum(x_real ** 2 + x_imag ** 2, axis=1)\n",
    "    mse = torch.sum((x_real - x_hat_real) ** 2 + (x_imag - x_hat_imag) ** 2, axis=1)\n",
    "    nmse = mse / power\n",
    "    return nmse\n",
    " \n",
    "class NMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='sum'):\n",
    "        super(NMSELoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    " \n",
    "    def forward(self, x_hat, x):\n",
    "        nmse = NMSE_cuda(x, x_hat)\n",
    "        if self.reduction == 'mean':\n",
    "            nmse = torch.mean(nmse)\n",
    "        else:\n",
    "            nmse = torch.sum(nmse)\n",
    "        return nmse\n",
    "\n",
    "def Score(NMSE):\n",
    "    score = 1 - NMSE\n",
    "    return score\n",
    "\n",
    "\n",
    "class DatasetFolder(Dataset):\n",
    "    def __init__(self, matData, phase='val'):\n",
    "        self.matdata = matData\n",
    "        self.phase = phase\n",
    "        self.data_shape = matData[0].shape\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        y = self.matdata[index]\n",
    "        return y\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.matdata.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T03:29:13.982008Z",
     "iopub.status.busy": "2022-02-16T03:29:13.981761Z",
     "iopub.status.idle": "2022-02-16T03:29:14.106074Z",
     "shell.execute_reply": "2022-02-16T03:29:14.105621Z",
     "shell.execute_reply.started": "2022-02-16T03:29:13.981989Z"
    },
    "id": "JhOyoJhhNhW9",
    "outputId": "f56772e0-1e7c-4cab-fb38-d7bb5594ff39",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "#CRnet论文中的cosine退火学习率\n",
    "class WarmUpCosineAnnealingLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_max, T_warmup, eta_min=0, last_epoch=-1):\n",
    "        self.T_max = T_max\n",
    "        self.T_warmup = T_warmup\n",
    "        self.eta_min = eta_min\n",
    "        super(WarmUpCosineAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.last_epoch < self.T_warmup:\n",
    "            return [base_lr * self.last_epoch / self.T_warmup for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            k = 1 + math.cos(math.pi * (self.last_epoch - self.T_warmup) / (self.T_max - self.T_warmup))\n",
    "            return [self.eta_min + (base_lr - self.eta_min) * k / 2 for base_lr in self.base_lrs]\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的初始化和训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-02-16T03:16:03.830854Z",
     "iopub.status.busy": "2022-02-16T03:16:03.830578Z",
     "iopub.status.idle": "2022-02-16T03:16:07.758644Z",
     "shell.execute_reply": "2022-02-16T03:16:07.758083Z",
     "shell.execute_reply.started": "2022-02-16T03:16:03.830836Z"
    },
    "id": "OaDPaLNoNkAN",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "72283d5f-e62b-41d7-a9c9-dbb626650cae",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2, 126, 128])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 2, 126, 128]             100\n",
      "       BatchNorm2d-2          [-1, 2, 126, 128]               4\n",
      "         LeakyReLU-3          [-1, 2, 126, 128]               0\n",
      "          Identity-4          [-1, 2, 126, 128]               0\n",
      "            Conv2d-5         [-1, 32, 126, 128]             576\n",
      "       BatchNorm2d-6         [-1, 32, 126, 128]              64\n",
      "         LeakyReLU-7         [-1, 32, 126, 128]               0\n",
      "            Conv2d-8         [-1, 32, 126, 128]           9,216\n",
      "       BatchNorm2d-9         [-1, 32, 126, 128]              64\n",
      "        LeakyReLU-10         [-1, 32, 126, 128]               0\n",
      "           Conv2d-11         [-1, 32, 126, 128]           9,216\n",
      "      BatchNorm2d-12         [-1, 32, 126, 128]              64\n",
      "           Conv2d-13         [-1, 32, 126, 128]             320\n",
      "      BatchNorm2d-14         [-1, 32, 126, 128]              64\n",
      "        LeakyReLU-15         [-1, 32, 126, 128]               0\n",
      "           Conv2d-16         [-1, 32, 126, 128]           5,120\n",
      "      BatchNorm2d-17         [-1, 32, 126, 128]              64\n",
      "AdaptiveAvgPool2d-18             [-1, 64, 1, 1]               0\n",
      "           Linear-19                    [-1, 8]             512\n",
      "      BatchNorm1d-20                    [-1, 8]              16\n",
      "             ReLU-21                    [-1, 8]               0\n",
      "           Linear-22                   [-1, 64]             512\n",
      "      BatchNorm1d-23                   [-1, 64]             128\n",
      "          Sigmoid-24                   [-1, 64]               0\n",
      "          SELayer-25         [-1, 64, 126, 128]               0\n",
      "        LeakyReLU-26         [-1, 64, 126, 128]               0\n",
      "           Conv2d-27          [-1, 2, 126, 128]             128\n",
      "      BatchNorm2d-28          [-1, 2, 126, 128]               4\n",
      "      BatchNorm2d-29          [-1, 2, 126, 128]               4\n",
      "        LeakyReLU-30          [-1, 2, 126, 128]               0\n",
      "          CRBlock-31          [-1, 2, 126, 128]               0\n",
      "           Linear-32                  [-1, 128]       4,128,896\n",
      "      BatchNorm1d-33                  [-1, 128]             256\n",
      "          Sigmoid-34                  [-1, 128]               0\n",
      "QuantizationLayer-35                  [-1, 512]               0\n",
      "          Encoder-36                  [-1, 512]               0\n",
      "DequantizationLayer-37                  [-1, 128]               0\n",
      "           Linear-38                [-1, 32256]       4,161,024\n",
      "          Sigmoid-39                [-1, 32256]               0\n",
      "           Conv2d-40         [-1, 64, 126, 128]           1,152\n",
      "      BatchNorm2d-41         [-1, 64, 126, 128]             128\n",
      "        LeakyReLU-42         [-1, 64, 126, 128]               0\n",
      "         Identity-43         [-1, 64, 126, 128]               0\n",
      "           Conv2d-44         [-1, 64, 126, 128]         102,400\n",
      "      BatchNorm2d-45         [-1, 64, 126, 128]             128\n",
      "        LeakyReLU-46         [-1, 64, 126, 128]               0\n",
      "           Conv2d-47         [-1, 64, 126, 128]         200,704\n",
      "      BatchNorm2d-48         [-1, 64, 126, 128]             128\n",
      "        LeakyReLU-49         [-1, 64, 126, 128]               0\n",
      "           Conv2d-50         [-1, 64, 126, 128]         331,776\n",
      "      BatchNorm2d-51         [-1, 64, 126, 128]             128\n",
      "        LeakyReLU-52         [-1, 64, 126, 128]               0\n",
      "           Conv2d-53         [-1, 64, 126, 128]          12,288\n",
      "      BatchNorm2d-54         [-1, 64, 126, 128]             128\n",
      "      BatchNorm2d-55         [-1, 64, 126, 128]             128\n",
      "        LeakyReLU-56         [-1, 64, 126, 128]               0\n",
      "         MRFBlock-57         [-1, 64, 126, 128]               0\n",
      "           Conv2d-58          [-1, 2, 126, 128]           1,154\n",
      "          Sigmoid-59          [-1, 2, 126, 128]               0\n",
      "          Decoder-60          [-1, 2, 126, 128]               0\n",
      "================================================================\n",
      "Total params: 8,966,594\n",
      "Trainable params: 8,966,594\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 212.15\n",
      "Params size (MB): 34.20\n",
      "Estimated Total Size (MB): 246.47\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130/1636482755.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  out = integer.unsqueeze(-1) // 2 ** exponent_bits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "learning_rate = 2e-3\n",
    "feedback_bits = 512\n",
    "\n",
    "model = AutoEncoder(feedback_bits,efn_name='tf_efficientnet_b0_ns')\n",
    "criterion = NMSELoss(reduction='mean')  # nn.MSELoss()\n",
    "criterion_test = NMSELoss(reduction='sum')\n",
    "\n",
    "train_dataset = DatasetFolder(x_train,phase = 'train')\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "test_dataset = DatasetFolder(x_test,phase = 'val')\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,num_workers=4)\n",
    "for d in train_loader:\n",
    "    print(d.shape)\n",
    "    break\n",
    "best_loss =100\n",
    "model = model.cuda()\n",
    "summary(model,(2,126,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 训练环境说明\n",
    "\n",
    "<br>CPU：2.5g   内存：8G   显卡：Ti3060   \n",
    "<br>epoch:500,训练时长:4hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-02-16T03:16:17.742395Z",
     "iopub.status.busy": "2022-02-16T03:16:17.741954Z",
     "iopub.status.idle": "2022-02-16T03:21:50.229009Z",
     "shell.execute_reply": "2022-02-16T03:21:50.228481Z",
     "shell.execute_reply.started": "2022-02-16T03:16:17.742374Z"
    },
    "id": "K4u7pVa9Nnwd",
    "outputId": "598df8ad-dcc6-440b-9356-f80f6081c0e6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================lr:3.0000e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130/1636482755.py:8: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  out = integer.unsqueeze(-1) // 2 ** exponent_bits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/250]\tLoss 0.03649970\t\n",
      "Epoch: [0][100/250]\tLoss 0.00031055\t\n",
      "Epoch: [0][200/250]\tLoss 0.00030923\t\n",
      "time for this epch: 154.34033346176147\n",
      "NMSE 1.3254   MSE 0.0000078281\n",
      "Model saved!\n",
      "========================lr:3.0000e-03\n",
      "Epoch: [1][0/250]\tLoss 0.00021043\t\n",
      "Epoch: [1][100/250]\tLoss 0.00023766\t\n",
      "Epoch: [1][200/250]\tLoss 0.00021129\t\n",
      "time for this epch: 159.75664925575256\n",
      "NMSE 1.0733   MSE 0.0000065440\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "modelSave1 = './data/models/encoder.pth.tar'\n",
    "modelSave2 ='./data/models/decoder.pth.tar'\n",
    "mse_fn = nn.MSELoss()\n",
    "learning_rate = 3e-3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate,weight_decay=0.05,betas=(0.9,0.95))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=4,  min_lr=5e-7, verbose=False)\n",
    "for epoch in range(epochs):\n",
    "    print('========================lr:%.4e' % optimizer.param_groups[0]['lr'])\n",
    "    # 训练\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    for i, input in enumerate(train_loader):\n",
    "        input = input.cuda()\n",
    "        output= model(input)\n",
    " \n",
    "        loss= mse_fn(output, input)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss:.8f}\\t'.format(\n",
    "                epoch, i, len(train_loader), loss=loss.item()))\n",
    "        running_loss += loss.item()\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    # 打印epoch信息\n",
    "    print('time for this epch:',time.time()-start_time)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    mse_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, input in enumerate(test_loader):\n",
    "            # convert numpy to Tensor\n",
    "            input = input.cuda()\n",
    "            output = model(input)\n",
    "            total_loss += criterion_test(output, input).item()\n",
    "            mse_loss += mse_fn(output, input).item()\n",
    "        average_loss = total_loss / len(test_dataset)\n",
    "        average_loss_mse = mse_loss / len(test_dataset)\n",
    "        print('NMSE %.4f' % average_loss,'  MSE %.10f' % average_loss_mse)\n",
    "        if average_loss < best_loss:\n",
    "            # model save\n",
    "            # save encoder\n",
    "            torch.save({'state_dict': model.encoder.state_dict(), }, modelSave1)\n",
    "            # save decoder\n",
    "            torch.save({'state_dict': model.decoder.state_dict(), }, modelSave2)\n",
    "            print('Model saved!')\n",
    "            best_loss = average_loss\n",
    "    scheduler.step(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.保存和加载model,以备提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ReOJTBC4Nor1"
   },
   "outputs": [],
   "source": [
    "modelSave1 = './data/models/encoder.pth.tar'\n",
    "torch.save({'state_dict': model.encoder.state_dict(), }, modelSave1)\n",
    "# save decoder\n",
    "modelSave2 ='./data/models/decoder.pth.tar'\n",
    "torch.save({'state_dict': model.decoder.state_dict(), }, modelSave2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1wqcTpDJpXJ",
    "outputId": "e2cfd13a-52a9-48b5-ae64-8a0cb8dd5ca7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.load_state_dict(torch.load('/content/drive/MyDrive/naic_models/encoderb1.pth.tar')['state_dict'])\n",
    "model.decoder.load_state_dict(torch.load('/content/drive/MyDrive/naic_models/decoderb1.pth.tar')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LLnWUwuZY7gg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AutoEncoder"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
